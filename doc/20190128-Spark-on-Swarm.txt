#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2015, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


###############################################################
## Creating a Docker Swarm and running a Spark Cluster on it ##
###############################################################


## Create Master VM 

createvm Gworewia (master)
..
Domain Gworewia started

ssh Stevedore@Gworewia

    ## Initialize Swarm with VM's address

    docker swarm init --advertise-addr 192.168.201.15

        ....
	Swarm initialized: current node (tkhj2d3z40x51vodr1a2b4n2h) is now a manager.

	To add a worker to this swarm, run the following command:

	    docker swarm join --token SWMTKN-1-66w8t157ufren0nv8kvdvh912va3y7wpzw6t7nbiz5xjt5vxbe-cvc3tg9bktu1likj06btd69x6 192.168.201.15:2377

	To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
        ....
    
    ## Fix Firewall issue
    sudo su
    firewall-cmd --add-port=2377/tcp --permanent
    firewall-cmd --reload



## ------------------------------------------------- ##


## Create Worker VM 
createvm Astoalith (worker)
Domain Astoalith started

ssh Stevedore@Astoalith

    docker swarm join --token SWMTKN-1-66w8t157ufren0nv8kvdvh912va3y7wpzw6t7nbiz5xjt5vxbe-cvc3tg9bktu1likj06btd69x6 192.168.201.15:2377
      ...
      This node joined a swarm as a worker.
      ...

      ## Fetch Spark Project source code (Need to do this on every Swarm node)
      git clone https://github.com/stvoutsin/lasair-experimental

## ------------------------------------------------- ##

## Connect to Master VM
ssh Stevedore@Gworewia

## Fetch Spark Project source code
pushd ${HOME}

    git clone https://github.com/stvoutsin/lasair-experimental

popd


## Deploy Spark Stack
pushd ${HOME}/lasair-experimental/spark/docker/compose/stack

    docker stack deploy -c docker-compose.yml spark
       ...

	Creating network spark_spark-net
	Creating service spark_spark-worker
	Creating service spark_spark-worker-2
	Creating service spark_spark-master

      ...

     docker ps
	CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                          NAMES
	85b15e4a4d10        bde2020/spark-master:2.4.0-hadoop2.7   "/bin/bash /master.sh"   13 minutes ago      Up 13 minutes       6066/tcp, 7077/tcp, 8080/tcp   spark_spark-master.1.2asigry7c8qaku970bhoscyke
	960e02adac3e        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   13 minutes ago      Up 13 minutes       8081/tcp                       spark_spark-worker-2.1.j028cnf2pmfsjgeuejpgevgjw
	0a66f1582b22        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   14 minutes ago      Up 13 minutes       8081/tcp                       spark_spark-worker.1.96qitf0kgu0szw1a8awqlj88j

     ## Connect to Spark Master and submit job
     docker exec -it 85b15e4a4d10 bash


popd


## Check Spark UI

## stelios@stelios-pc
ssh -L '*:8084:Gworewia:8080' Stevedore@Gworewia
(Open browser at http://localhost:8084..)

... 
	URL: spark://85b15e4a4d10:7077
	Alive Workers: 2
	Cores in use: 8 Total, 0 Used
...


## Scaling Workers
# We can also scale up the number of Spark workers as follows:
  docker service scale spark_spark-worker=4

# These correctly show up in Spark UI as Spark worker nodes, and are distributed using Swarm on the Worker VMs
# So perhaps better to do this than define multiple workers in docker compose
 
