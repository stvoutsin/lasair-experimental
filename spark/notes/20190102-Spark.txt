#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2015, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

ssh Stevedore@Froeseth 

### PySpark using BDE images

	### Start Spark Master
	 docker run --name spark-master -h spark-master -p 8080:8080 -e ENABLE_INIT_DAEMON=false -d bde2020/spark-master:2.4.0-hadoop2.7

	### Start Spark Workers
	 docker run --name spark-worker-1 --link spark-master:spark-master -e ENABLE_INIT_DAEMON=false -d bde2020/spark-worker:2.4.0-hadoop2.7
	 docker run --name spark-worker-2 --link spark-master:spark-master -e ENABLE_INIT_DAEMON=false -d bde2020/spark-worker:2.4.0-hadoop2.7
	 docker run --name spark-worker-3 --link spark-master:spark-master -e ENABLE_INIT_DAEMON=false -d bde2020/spark-worker:2.4.0-hadoop2.7


[Stevedore@Froeseth ~]$ docker ps
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                                        NAMES
4ca9d359eeb4        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   19 hours ago        Up 19 hours         8081/tcp                                     spark-worker-2
bddeb818449b        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   19 hours ago        Up 19 hours         8081/tcp                                     spark-worker-1
c0788374013b        bde2020/spark-master:2.4.0-hadoop2.7   "/bin/bash /master.sh"   19 hours ago        Up 19 hours         6066/tcp, 7077/tcp, 0.0.0.0:8080->8080/tcp   spark-master


### PySpark using BDE images
root@spark-master:/app# cat  >  /spark/conf/spark-defaults.conf  << EOF

	spark.executor.cores=1
	spark.driver.cores=1
	spark.driver.memory=600m
	spark.executor.memory=580m

EOF


## Tried connecting to cluster via spark-submit client, but can not get that working..
## It does work when connecting via spark-master spark


docker exec -it spark-master bash


cat > "/app/simple_app.py" << EOF
                                                                                                     

from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

import time
from operator import add

conf = SparkConf()
conf.setMaster('spark://spark-master:7077')
conf.setAppName('spark-basic')
sc = SparkContext(conf=conf)

data = sc.parallelize(list("Hello World"))
counts = data.map(lambda x: (x, 1)).reduceByKey(add).sortBy(lambda x: x[1], ascending=False).collect()
for (word, count) in counts:
    print("{}: {}".format(word, count))
sc.stop()

EOF





## Connecting PySpark to Lasair DB

## Step 1: Download mysql-connector-java-8.0.13.jar
## Step 2: Copy Connector into master and all workers
## .. docker cp /home/Stevedore/app/mysql-connector-java-8.0.13.jar spark-worker-3:/spark/jars/mysql-connector-java-8.0.13.jar


# In Spark master ..
mkdir /app/second_htm
cd /app/second_htm
## Get htmCircle.py and _htmCircle.so 
    wget http://dark.physics.ucdavis.edu/~hal/FlaskApp/FlaskApp/htmCircle.py
    wget http://dark.physics.ucdavis.edu/~hal/FlaskApp/FlaskApp/_htmCircle.so


## Create app in master and run using spark-submit

cat > "/app/watchlists2.py" << EOF

from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import to_timestamp
import sys
import time
import math
from operator import add
sys.path.append('/app/second_htm')
#sys.path.append('/app/htmCircleApp')

import htmCircle


conf = SparkConf()
conf.setMaster('spark://spark-master:7077')
conf.setAppName('spark-basic')
sc = SparkContext(conf=conf)

point = [336.14, 0.13]

properties = {
    'user'    : '',       
    'password': '',
    'host'    : 'jdbc:mysql://:3306',
    'database': '',
    'driver': 'com.mysql.jdbc.Driver',
    'url' : 'jdbc:mysql://:3306'
}

working_directory = '/app/'


def distance(ra1, de1, ra2, de2):
    dra = (ra1 - ra2)*math.cos(de1*math.pi/180)
    dde = (de1 - de2)
    return math.sqrt(dra*dra + dde*dde)


spark = SparkSession \
    .builder \
    .appName('pyspark_demo_app') \
    .config('spark.driver.extraClassPath',
            working_directory + 'mysql-connector-java-8.0.13.jar') \
    .master("local[*]") \
    .getOrCreate() 

def filterHits(row, wl_radius, myRA, myDecl, wl_id, cone_id, name):

    objectId = row['objectId']
    ndethist = row['ncand']
    arcsec = 3600*distance(myRA, myDecl, row['ramean'], row['decmean'])
    if arcsec <= wl_radius:
        return {
                'wl_id':wl_id,
                'cone_id':cone_id,
                'name':name,
                'objectId':objectId,
                'ndethist':ndethist,
                'arcsec':arcsec
        }
    else :
        return None


def run_watchlist(wl_id, delete_old=True):

    newhitlist = []

    df1 = spark.read \
        .format('jdbc') \
        .option('driver', properties['driver']) \
        .option('url', properties['url']) \
        .option('user', properties['user']) \
        .option('password', properties['password']) \
        .option('dbtable', 'watchlists') \
        .load()

    df1.createOrReplaceTempView("watchlists")

    # get information about the watchlist we are running
    query = 'SELECT name,radius FROM watchlists WHERE wl_id=%d' % wl_id
    query_results = spark.sql(query)
    watchlists = query_results.rdd.map(lambda p: {'name' : p.name, 'radius' : p.radius}).collect()
    newhitlist = []

    for watchlist in watchlists:
        wl_name   = watchlist['name']
        wl_radius = watchlist['radius']

    print ("Running Watchlist '%s' at radius %.1f" % (wl_name, wl_radius))

    # make a list of all the hits to return it
    newhitlist = []


    df2 = spark.read \
        .format('jdbc') \
        .option('driver', properties['driver']) \
        .option('url', properties['url']) \
        .option('user', properties['user']) \
        .option('password', properties['password']) \
        .option('dbtable', 'watchlist_cones') \
        .load()


    df2.createOrReplaceTempView("watchlist_cones")
    
    # get all the cones and run them
    query2 = 'SELECT cone_id,name,ra,decl FROM watchlist_cones WHERE wl_id=%d' % wl_id
    query_results2 = spark.sql(query2)
    cones = query_results2.rdd.map(lambda p: {'cone_id' : p.cone_id, 'name' : p.name, 'ra' : p.ra, 'decl' : p.decl }).collect()

    for watch_pos in cones:
        cone_id  = watch_pos['cone_id']
        name     = watch_pos['name']
        myRA     = watch_pos['ra']
        myDecl   = watch_pos['decl']
    
        subClause = htmCircle.htmCircleRegion(16, myRA, myDecl, wl_radius)
        subClause = subClause.replace('htm16ID', 'htm16')


        df4 = spark.read \
            .format('jdbc') \
            .option('driver', properties['driver']) \
            .option('url', properties['url']) \
            .option('user', properties['user']) \
            .option('password', properties['password']) \
            .option('dbtable', 'objects') \
            .load()

        query3 = 'SELECT * FROM objects WHERE htm16 ' + subClause[14: -2]
        
        df4.createOrReplaceTempView("objects")
        query_results3 = spark.sql(query3)
        newhitlist.append(query_results3.rdd.map(lambda p: filterHits(p, wl_radius, myRA, myDecl, wl_id, cone_id, name)).collect())
    newhitlist = [i for i in newhitlist if i != []]
    return newhitlist


wl_id = 35
start = time. time()
hitlist = run_watchlist(wl_id)
print('Found %d matches' % len(hitlist))
end = time. time()
print("Time taken: " + str(end - start) + " seconds")

EOF








cat > "/app/watchlists3.py" << EOF


from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import to_timestamp
import sys
import time
import math
from operator import add
import logging
sys.path.append('/app/second_htm')
#sys.path.append('/app/htmCircleApp')

import htmCircle


conf = SparkConf()
conf.setMaster('spark://spark-master:7077')
conf.setAppName('spark-basic')
sc = SparkContext(conf=conf)
sc.addPyFile("/app/second_htm/htmCircle.py") 
sc.addPyFile("/app/second_htm/_htmCircle.so") 

point = [336.14, 0.13]

properties = {
    'user'    : '',       
    'password': '',
    'host'    : '',
    'database': '',
    'driver': 'com.mysql.jdbc.Driver',
    'url' : ''
}

working_directory = '/app/'


def distance(ra1, de1, ra2, de2):
    dra = (ra1 - ra2)*math.cos(de1*math.pi/180)
    dde = (de1 - de2)
    return math.sqrt(dra*dra + dde*dde)


spark = SparkSession \
    .builder \
    .appName('pyspark_demo_app') \
    .config('spark.driver.extraClassPath',
            working_directory + 'mysql-connector-java-8.0.13.jar') \
	    .master("local[*]") \
    .getOrCreate() 


def cone_crossmatch(cone, wl_radius, df):

    cone_id  = cone['cone_id']
    name     = cone['name']
    myRA     = cone['ra']
    myDecl   = cone['decl']
    
    subClause = htmCircle.htmCircleRegion(16, myRA, myDecl, wl_radius)
    subClause = subClause.replace('htm16ID', 'htm16').replace('where', '')

    return  df.select('*').filter(subClause).rdd.map(lambda p : filter_hits(p, wl_radius, myRA, myDecl, wl_id, cone_id, name)).collect()  
    #query3 = 'SELECT * FROM objects WHERE htm16 ' + subClause[14: -2]
    #query_results3 = spark.sql(query3)
    #return query_results3.rdd.map(lambda p: filter_hits(p, wl_radius, myRA, myDecl, wl_id, cone_id, name)).collect()


def filter_hits(row, wl_radius, myRA, myDecl, wl_id, cone_id, name):

    objectId = row['objectId']
    ndethist = row['ncand']
    arcsec = 3600*distance(myRA, myDecl, row['ramean'], row['decmean'])

    if arcsec <= wl_radius:
        return {
                'wl_id':wl_id,
                'cone_id':cone_id,
                'name':name,
                'objectId':objectId,
                'ndethist':ndethist,
                'arcsec':arcsec
        }
    else :
        return None


def run_watchlist(wl_id, delete_old=True):

    newhitlist = []

    df1 = spark.read \
        .format('jdbc') \
        .option('driver', properties['driver']) \
        .option('url', properties['url']) \
        .option('user', properties['user']) \
        .option('password', properties['password']) \
        .option('dbtable', 'watchlists') \
        .load()

    df1.createOrReplaceTempView("watchlists")

    # get information about the watchlist we are running
    # query = 'SELECT name,radius FROM watchlists WHERE wl_id=%d' % wl_id
    # query_results = spark.sql(query)
    # watchlists = query_results.rdd.map(lambda p: {'name' : p.name, 'radius' : p.radius}).collect()
    watchlists = df1.select(["name", "radius"]).filter(df1.wl_id == wl_id).collect()

    for watchlist in watchlists:
        wl_name   = watchlist['name']
        wl_radius = watchlist['radius']

    print ("Running Watchlist '%s' at radius %.1f" % (wl_name, wl_radius))

    # make a list of all the hits to return it
    newhitlist = []
	
    df2 = spark.read \
        .format('jdbc') \
        .option('driver', properties['driver']) \
        .option('url', properties['url']) \
        .option('user', properties['user']) \
        .option('password', properties['password']) \
        .option('dbtable', 'watchlist_cones') \
        .load()

    df2.createOrReplaceTempView("watchlist_cones")
    
    # get all the cones and run them
    #query2 = 'SELECT cone_id,name,ra,decl FROM watchlist_cones WHERE wl_id=%d' % wl_id	
    #query_results2 = spark.sql(query2)
    #cones = query_results2.rdd.map(lambda p: {'cone_id' : p.cone_id, 'name' : p.name, 'ra' : p.ra, 'decl' : p.decl })
  
    cones = df2.select(["cone_id","name", "ra", "decl"]).filter(df2.wl_id == wl_id).collect()

    df4 = spark.read \
        .format('jdbc') \
        .option('driver', properties['driver']) \
        .option('url', properties['url']) \
        .option('user', properties['user']) \
        .option('password', properties['password']) \
        .option('dbtable', 'objects') \
        .load() 

    df4.createOrReplaceTempView("objects")

    # newhitlist = cones.reduceByKey(lambda x : cone_crossmatch(x, wl_radius)).collect() 

    
    #newhitlist = query_results2.rdd.map(lambda p : cone_crossmatch(p, wl_radius)).collect()
    for cone in cones:
        newhitlist.append(cone_crossmatch(cone, wl_radius, df4))


    newhitlist = [i for i in newhitlist if i != []]
    return newhitlist


wl_id = 35
start = time. time()
hitlist = run_watchlist(wl_id)
print('Found %d matches' % len(hitlist))
print (hitlist)
end = time. time()
print("Time taken: " + str(end - start) + " seconds")

EOF
